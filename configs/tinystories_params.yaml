train:
  train_data: /root/autodl-tmp/tinystories/train.bin
  valid_data: /root/autodl-tmp/tinystories/valid.bin
  data_dtype: uint16
  out_dir: /root/autodl-tmp/runs/exp_train_lr3e-3-min3e-5-15000-3
  resume_from: /root/autodl-tmp/runs/exp_train_lr3e-3-min3e-5-15000-3/checkpoint_step_10000.pt

  vocab_size: 10000
  context_length: 256
  d_model: 512
  d_ff: 1344
  num_layers: 4
  num_heads: 16
  rope_theta: 10000.0

  optimizer: adamw
  learning_rate: 3e-3
  min_learning_rate: 3e-5
  warmup_iters: 750
  cosine_cycle_iters: null
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8
  weight_decay: 0.1
  grad_clip: 1.0

  batch_size: 32
  max_steps: 15000
  log_interval: 10
  eval_interval: 2000
  eval_iters: 50
  ckpt_interval: 5000

  no_rmsnorm: false
  norm_style: pre
  no_rope: false
  ffn_type: swiglu

  device: auto
  seed: 42
  matmul_precision: high
  compile: false
  compile_backend: null

  wandb_project: "tinystories"
  wandb_name: exp_train_lr3e-3-min3e-5-15000-3

train-from-text:
  train_text: data/TinyStoriesV2-GPT4-train.txt
  valid_text: data/TinyStoriesV2-GPT4-valid.txt
  special_tokens: ["<|endoftext|>"]
  data_dtype: uint16
  out_dir: runs/exp_train
  overwrite: false
  resume_from: null
  encode_workers: 8
  encode_backend: pool
  encode_slot_tokens: 0
  encode_num_slots: 0

  vocab_size: 10000
  context_length: 256
  d_model: 512
  d_ff: 1344
  num_layers: 4
  num_heads: 16
  rope_theta: 10000.0

  optimizer: adamw
  learning_rate: 0.001
  min_learning_rate: 0.0001
  warmup_iters: 200
  cosine_cycle_iters: null
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8
  weight_decay: 0.1
  grad_clip: 1.0

  batch_size: 32
  max_steps: 4000
  log_interval: 10
  eval_interval: 200
  eval_iters: 50
  ckpt_interval: 500

  no_rmsnorm: false
  norm_style: pre
  no_rope: false
  ffn_type: swiglu

  device: auto
  seed: 42
  matmul_precision: null
  compile: false
  compile_backend: null

  wandb_project: "tinystories"
  wandb_name: exp_train_lr3e-3-min3e-5-15000-2

generate:
  checkpoint: runs/exp_train/checkpoint_final.pt
  vocab_path: runs/exp_train/vocab.json
  merges_path: runs/exp_train/merges.txt
  special_tokens: ["<|endoftext|>"]

  context_length: 256
  d_model: 512
  d_ff: 1344
  num_layers: 4
  num_heads: 16
  rope_theta: 10000.0

  no_rmsnorm: false
  norm_style: pre
  no_rope: false
  ffn_type: swiglu

  prompt: "Once upon a time"
  max_new_tokens: 256
  temperature: 1.0
  top_p: 0.9
  top_k: 0
  repetition_penalty: 1.0
  stop_strings: []
  eos_token: "<|endoftext|>"

  seed: 42
  device: auto
  matmul_precision: null
  compile: false
  compile_backend: null
